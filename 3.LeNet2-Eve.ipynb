{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sara/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/sara/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/home/sara/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/sara/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 6148520152113955261\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sara/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.test.gpu_device_name())\n",
    "# See https://www.tensorflow.org/tutorials/using_gpu#allowing_gpu_memory_growth\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/sara/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/sara/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.layers.core import Activation, Flatten, Dense, Dropout\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers import Input\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.applications.xception import Xception, preprocess_input\n",
    "from keras.regularizers import l2\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "import tflearn\n",
    "import numpy as np\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.normalization import local_response_normalization\n",
    "from alexnet_additional_layers import split_tensor, cross_channel_normalization\n",
    "from decode_predictions import decode_classnames_json, decode_classnumber\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from Eve import Eve\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'data/Reinhard'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/training 0\n",
      "data/training/Invasive 7\n",
      "data/training/Normal 7\n",
      "data/training/InSitu 7\n",
      "data/training/Benign 7\n"
     ]
    }
   ],
   "source": [
    "for root,dirs,files in os.walk(train_dir):\n",
    "    print (root, len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_size = (256, 256)\n",
    "img_rows, img_cols = (256, 256)\n",
    "nb_classes = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24 images belonging to 4 classes.\n",
      "Found 4 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        fill_mode='nearest',\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.25)\n",
    "\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "\n",
    "#target_size: Tuple of integers (height, width), default: (256, 256). \n",
    "#The dimensions to which all images found will be resized.\n",
    "\n",
    "#target_size = (height, width)\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size = target_size,       \n",
    "        class_mode = 'categorical',\n",
    "        batch_size=32,\n",
    "        subset=\"training\",\n",
    "        shuffle = True)\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size = target_size,        \n",
    "        class_mode = 'categorical',\n",
    "        batch_size=32,\n",
    "        subset = \"validation\",\n",
    "        shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Intilizing variables\n",
    "output_classes = 4\n",
    "batch_size = 32 \n",
    "epochs = 30\n",
    "\n",
    "sgd_opt = SGD(lr=1E-2, decay=1E-4, momentum=0.9, nesterov=True)\n",
    "adam_opt = Adam(lr=1E-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=1E-4)\n",
    "eve_opt = Eve(lr=1E-4, decay=1E-4, beta_1=0.9, beta_2=0.999, beta_3=0.999, small_k=0.1, big_K=10, epsilon=1e-08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Lenet2():\n",
    "    x_input = Input(shape=(img_rows, img_cols, 3), name=\"input\")\n",
    "    x = Conv2D(32, (3, 3))(x_input)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D(pool_size = (2, 2))(x)\n",
    "    # second layer CONV => RELU => POOL\n",
    "    x = Conv2D(32, (3, 3))(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D(pool_size = (2, 2))(x)\n",
    "    # third layer of CONV => RELU => POOL\n",
    "    x = Conv2D(64, (3, 3))(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D(pool_size = (2, 2))(x)\n",
    "    # set of FC => RELU layers\n",
    "    x = Flatten()(x)\n",
    "    # number of neurons in FC layer = 128\n",
    "    x = Dense(128)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    # as number of classes is 36\n",
    "    x = Dense(nb_classes)(x)\n",
    "    x = Activation('softmax')(x)\n",
    "\n",
    "    Lenet2 = Model(inputs=[x_input], outputs=[x])\n",
    "    return Lenet2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 256, 256, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 254, 254, 32)      896       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 254, 254, 32)      0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 254, 254, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 127, 127, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 125, 125, 32)      9248      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 125, 125, 32)      0         \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 125, 125, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 62, 62, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 60, 60, 64)        18496     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 60, 60, 64)        0         \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 60, 60, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 30, 30, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 57600)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               7372928   \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 516       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 7,402,084\n",
      "Trainable params: 7,402,084\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Lenet2()\n",
    "model.summary()\n",
    "model.compile(loss = 'categorical_crossentropy',\n",
    "              optimizer = eve_opt,\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_train_samples= len(train_generator.filenames)\n",
    "num_train_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_val_samples= len(validation_generator.filenames)\n",
    "num_val_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Variable *= will be deprecated. Use variable.assign_mul if you want assignment to the variable value or 'x = x * y' if you want a new python Tensor object.\n",
      "Epoch 1/30\n",
      "1/1 [==============================] - 6s 6s/step - loss: 1.3715 - acc: 0.2500 - val_loss: 1.4362 - val_acc: 0.2500\n",
      "Epoch 2/30\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.7409 - acc: 0.2917 - val_loss: 1.3873 - val_acc: 0.2500\n",
      "Epoch 3/30\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.5104 - acc: 0.2500 - val_loss: 1.4385 - val_acc: 0.2500\n",
      "Epoch 4/30\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.6258 - acc: 0.3333 - val_loss: 1.4257 - val_acc: 0.2500\n",
      "Epoch 5/30\n",
      "1/1 [==============================] - 4s 4s/step - loss: 1.7942 - acc: 0.2083 - val_loss: 1.3879 - val_acc: 0.2500\n",
      "Epoch 6/30\n",
      "1/1 [==============================] - 4s 4s/step - loss: 1.7436 - acc: 0.2083 - val_loss: 1.3790 - val_acc: 0.2500\n",
      "Epoch 7/30\n",
      "1/1 [==============================] - 4s 4s/step - loss: 1.5180 - acc: 0.2083 - val_loss: 1.4031 - val_acc: 0.2500\n",
      "Epoch 8/30\n",
      "1/1 [==============================] - 4s 4s/step - loss: 1.5275 - acc: 0.2500 - val_loss: 1.4101 - val_acc: 0.2500\n",
      "Epoch 9/30\n",
      "1/1 [==============================] - 4s 4s/step - loss: 1.6676 - acc: 0.3750 - val_loss: 1.4016 - val_acc: 0.2500\n",
      "Epoch 10/30\n",
      "1/1 [==============================] - 4s 4s/step - loss: 1.5395 - acc: 0.2917 - val_loss: 1.3890 - val_acc: 0.2500\n",
      "Epoch 11/30\n",
      "1/1 [==============================] - 4s 4s/step - loss: 1.5260 - acc: 0.2917 - val_loss: 1.3831 - val_acc: 0.2500\n",
      "Epoch 12/30\n",
      "1/1 [==============================] - 4s 4s/step - loss: 1.5202 - acc: 0.1667 - val_loss: 1.3754 - val_acc: 0.2500\n",
      "Epoch 13/30\n",
      "1/1 [==============================] - 4s 4s/step - loss: 1.3365 - acc: 0.2500 - val_loss: 1.3777 - val_acc: 0.2500\n",
      "Epoch 14/30\n",
      "1/1 [==============================] - 5s 5s/step - loss: 1.3603 - acc: 0.2500 - val_loss: 1.3769 - val_acc: 0.2500\n",
      "Epoch 15/30\n",
      "1/1 [==============================] - 5s 5s/step - loss: 1.4702 - acc: 0.2500 - val_loss: 1.3827 - val_acc: 0.2500\n",
      "Epoch 16/30\n",
      "1/1 [==============================] - 4s 4s/step - loss: 1.4206 - acc: 0.2917 - val_loss: 1.3770 - val_acc: 0.2500\n",
      "Epoch 17/30\n",
      "1/1 [==============================] - 5s 5s/step - loss: 1.4504 - acc: 0.2500 - val_loss: 1.3759 - val_acc: 0.2500\n",
      "Epoch 18/30\n",
      "1/1 [==============================] - 4s 4s/step - loss: 1.4308 - acc: 0.2083 - val_loss: 1.3741 - val_acc: 0.7500\n",
      "Epoch 19/30\n",
      "1/1 [==============================] - 5s 5s/step - loss: 1.2947 - acc: 0.4167 - val_loss: 1.3767 - val_acc: 0.2500\n",
      "Epoch 20/30\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(train_generator,\n",
    "        steps_per_epoch =int(np.ceil(num_train_samples * 1.0 / batch_size)),\n",
    "        epochs = epochs,\n",
    "        validation_data = validation_generator,\n",
    "        validation_steps=int(np.ceil(num_val_samples * 1.0 / batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history[\"val_loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history[\"val_acc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history[\"loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history[\"acc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate_generator(validation_generator, steps=50)\n",
    "\n",
    "print ('Test Score: ', score[0])\n",
    "print ('Test Accuracy: ',score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "plt.figure(1)\n",
    "plt.plot(history.history['loss'],'r')\n",
    "plt.plot(history.history['val_loss'],'g')\n",
    "plt.rcParams['figure.figsize'] = (8, 6)\n",
    "plt.xlabel(\"Num of Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss vs Validation Loss\")\n",
    "plt.legend(['train','validation'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = validation_generator.filenames\n",
    "truth = validation_generator.classes\n",
    "label = validation_generator.class_indices\n",
    "indexlabel = dict((value, key) for key, value in label.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts = model.predict_generator(validation_generator, steps=validation_generator.samples/validation_generator.batch_size, verbose=1)\n",
    "predict_class = np.argmax(predicts, axis=1)\n",
    "errors = np.where(predict_class != truth)[0]\n",
    "print(\"No of errors = {}/{}\".format(len(errors),validation_generator.samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(truth,predict_class)\n",
    "\n",
    "labels = []\n",
    "for k,v in indexlabel.items():\n",
    "    labels.append(v)\n",
    "    \n",
    "import itertools\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion Matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion Matrix')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plot_confusion_matrix(cm, classes=labels,\n",
    "                      title='Confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score \n",
    "\n",
    "#and reports metrics with classification_report method\n",
    "def predict_and_report(gen, model):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    gen.reset()\n",
    "    for img, label in gen:\n",
    "        #get true labels for batch and store them\n",
    "        y_true.extend([int(z[1]) for z in label])\n",
    "        #Get predictions as probabilities\n",
    "        batch_pred = model.predict_on_batch(img)\n",
    "        #turn probabilities to class labels and store\n",
    "        batch_pred = np.argmax(batch_pred, axis=1)\n",
    "        y_pred.extend(batch_pred)\n",
    "        #break loop\n",
    "        if gen.batch_index == 0:\n",
    "            break\n",
    "            \n",
    "    print('Accuracy:', accuracy_score(y_true, y_pred))\n",
    "    print('Area Under the Receiver Operating Characteristic Curve:', roc_auc_score(y_true, y_pred)) #Area under the curve\n",
    "    print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_and_report(train_generator, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_and_report(validation_generator, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_train_acc = max(history.history['acc'])\n",
    "best_train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_train_acc = history.history['acc'][-1]\n",
    "last_train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('models/3.LeNet2-Eve-Model.h5')\n",
    "model.save_weights('models/3.LeNet2-Eve-Weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
